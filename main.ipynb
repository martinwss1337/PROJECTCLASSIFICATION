{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: click in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\martin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: lxml in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\martin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install lxml\n",
    "%pip install -U scikit-learn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import email\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_18528\\990443069.py:21: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(s, 'lxml')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wrong bill grace forward original message rodr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>continued hilcorp old ocean deal dan hyvl writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>several related issue resulted increase level ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>one year rate one mm volume greater mm day pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>attached weekly deal report lex carroll enron ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             origin\n",
       "0      0  wrong bill grace forward original message rodr...\n",
       "1      0  continued hilcorp old ocean deal dan hyvl writ...\n",
       "2      0  several related issue resulted increase level ...\n",
       "3      0  one year rate one mm volume greater mm day pri...\n",
       "4      0  attached weekly deal report lex carroll enron ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "def read_email_from_string(s):\n",
    "    message = email.message_from_string(s)\n",
    "    return message\n",
    "\n",
    "def extract_email_body(message):\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            type_content = part.get_content_maintype()\n",
    "            if type_content == 'text':\n",
    "                message = part\n",
    "                break\n",
    "        else:\n",
    "            return ''\n",
    "    body = message.get_payload(decode=False)\n",
    "    return body\n",
    "\n",
    "def remove_html(s):\n",
    "    soup = BeautifulSoup(s, 'lxml')\n",
    "    for sp in soup(['script', 'style', 'head', 'meta', 'noscript', 'http']):\n",
    "        sp.decompose()\n",
    "    s = ' '.join(soup.stripped_strings)\n",
    "    return s\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Extract email body\n",
    "    body = extract_email_body(read_email_from_string(text))\n",
    "    body = remove_html(body)\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(body)\n",
    "\n",
    "    # Remove punctuation and convert to lower case\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    words = [word for word in words if word != 'subject']\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['origin'] = df['origin'].apply(preprocess_text)\n",
    "\n",
    "# Save the refactored emails back to the same file\n",
    "df.to_csv('preprocessed.csv', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10111"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1, random_state = 1)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Create split index for 70/30 split\n",
    "split_index = int(len(df) * 0.7)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = df[:split_index], df[split_index:]\n",
    "\n",
    "validation_split_index = int(len(train_df) * 0.8)\n",
    "train_df, val_df = train_df[:validation_split_index], df[validation_split_index:]\n",
    "\n",
    "\n",
    "df = train_df.reset_index(drop = True)\n",
    "df = test_df.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18872\n",
      "14830\n",
      "10111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens in training set\n",
    "token_counter = {}\n",
    "for message in train_df['origin']:\n",
    "  words = nltk.word_tokenize(message)\n",
    "\n",
    "  for token in words:\n",
    "    if token in token_counter:\n",
    "      token_counter[token] += 1\n",
    "    else:\n",
    "      token_counter[token] = 1\n",
    "\n",
    "token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_token(proccessed_token, threshold):\n",
    "  if proccessed_token not in token_counter:\n",
    "    return False\n",
    "  else:\n",
    "    # Add condition to check length of token\n",
    "    return token_counter[proccessed_token] > threshold and len(proccessed_token) > 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the current dataset 500 is a good value which generates a\n",
    "# set with an acceptable length to be considered a feature set for the\n",
    "# machine learning algorithms\n",
    "\n",
    "# this is the Bag of Words approach\n",
    "\n",
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 6000):\n",
    "    features.add(token)\n",
    "\n",
    "features = list(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = nltk.word_tokenize(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "\n",
    "  return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_to_count_vector(train_df['origin'].iloc[9010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 spam; 0 not spam\n",
    "train_df.iloc[9010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(dataframe):\n",
    "  # Extract labels and convert to integer type\n",
    "  labels = dataframe['label'].values.astype(int)\n",
    "\n",
    "  # Extract messages\n",
    "  messages = dataframe['origin']\n",
    "  vector_counts = []\n",
    "\n",
    "  # Convert each message to a count vector\n",
    "  for msg in messages:\n",
    "    vector = message_to_count_vector(msg)\n",
    "    vector_counts.append(vector)\n",
    "\n",
    "  # Convert list of count vectors to a numpy array and cast to integer type\n",
    "  feature_matrix = np.asarray(vector_counts).astype(int)\n",
    "\n",
    "  return feature_matrix, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18872, 13), (18872,), (10111, 13), (10111,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_BOW, Y_train_BOW = extract_features_and_labels(train_df)\n",
    "\n",
    "X_val_BOW, Y_val_BOW = extract_features_and_labels(val_df)\n",
    "\n",
    "X_test_BOW, Y_test_BOW = extract_features_and_labels(test_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_BOW.shape, Y_train_BOW.shape, X_test_BOW.shape, Y_test_BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_BOW)\n",
    "\n",
    "X_train_BOW, X_test_BOW, X_val_BOW = scaler.transform(X_train_BOW), scaler.transform(X_test_BOW), scaler.transform(X_val_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Best Score: 0.7590648174926719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.66      0.74      3330\n",
      "           1       0.72      0.86      0.79      3411\n",
      "\n",
      "    accuracy                           0.76      6741\n",
      "   macro avg       0.77      0.76      0.76      6741\n",
      "weighted avg       0.77      0.76      0.76      6741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "X_train_val_BOW = np.concatenate((X_train_BOW, X_val_BOW), axis=0)\n",
    "Y_train_val_BOW = np.concatenate((Y_train_BOW, Y_val_BOW), axis=0)\n",
    "\n",
    "lrBOW = LogisticRegression(solver='liblinear', max_iter=30000)\n",
    "grid_search = GridSearchCV(lrBOW, hyperparameters, cv=5)\n",
    "best_model = grid_search.fit(X_train_val_BOW, Y_train_val_BOW)\n",
    "best_params = best_model.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "print('Best Score:', best_model.best_score_)\n",
    "X_train_BOW, X_val_BOW, Y_train_BOW, Y_val_BOW = train_test_split(X_train_val_BOW, Y_train_val_BOW, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train the model with the best parameters\n",
    "lrBOW = LogisticRegression(solver='liblinear', max_iter=30000, **best_params)\n",
    "lrBOW.fit(X_train_BOW, Y_train_BOW)\n",
    "print(classification_report(Y_val_BOW, lrBOW.predict(X_val_BOW)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels_tfidf(dataframe):\n",
    "  # Extract labels and convert to integer type\n",
    "  labels = dataframe['label'].values.astype(int)\n",
    "\n",
    "  # Extract messages\n",
    "  messages = dataframe['origin']\n",
    "\n",
    "  # Use TFIDF vectorizer to convert messages to TFIDF vectors\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  tfidf_matrix = vectorizer.fit_transform(messages)\n",
    "\n",
    "  return tfidf_matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18872, 103048)\n",
      "(14830, 89592)\n",
      "(10111, 73849)\n"
     ]
    }
   ],
   "source": [
    "X_train_TFIDF, Y_train_TFIDF = extract_features_and_labels_tfidf(train_df)\n",
    "\n",
    "X_val_TFIDF, Y_val_TFIDF = extract_features_and_labels_tfidf(val_df)\n",
    "\n",
    "X_test_TFIDF, Y_test_TFIDF = extract_features_and_labels_tfidf(test_df)\n",
    "\n",
    "print(X_train_TFIDF.shape)\n",
    "print(X_val_TFIDF.shape)\n",
    "print(X_test_TFIDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_val_TFIDF \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_TFIDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_TFIDF\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m Y_train_val_TFIDF \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((Y_train_TFIDF, Y_val_TFIDF), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m lrTFIDF \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "X_train_val_TFIDF = np.concatenate((X_train_TFIDF, X_val_TFIDF), axis=0)\n",
    "Y_train_val_TFIDF = np.concatenate((Y_train_TFIDF, Y_val_TFIDF), axis=0)\n",
    "\n",
    "lrTFIDF = LogisticRegression(solver='liblinear', max_iter=30000)\n",
    "\n",
    "grid_search = GridSearchCV(lrTFIDF, hyperparameters, cv=5)\n",
    "\n",
    "best_model = grid_search.fit(X_train_val_TFIDF, Y_train_val_TFIDF)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = best_model.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "print('Best Score:', best_model.best_score_)\n",
    "\n",
    "# Split the sets again\n",
    "X_train_TFIDF, X_val_TFIDF, Y_train_TFIDF, Y_val_TFIDF = train_test_split(X_train_val_TFIDF, Y_train_val_TFIDF, test_size=0.2, random_state=42)\n",
    "\n",
    "lrTFIDF = LogisticRegression(solver='liblinear', max_iter=30000, **best_params)\n",
    "lrTFIDF.fit(X_train_TFIDF, Y_train_TFIDF)\n",
    "\n",
    "print(classification_report(Y_val_TFIDF, lrTFIDF.predict(X_val_TFIDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrTFIDF = LogisticRegression()\n",
    "\n",
    "lrTFIDF.fit(X_train_features, Y_train_TFIDF)\n",
    "\n",
    "prediction_training_data = lrTFIDF.predict(X_train_features)\n",
    "\n",
    "accuracy_training_data = accuracy_score(Y_train_TFIDF, prediction_training_data)\n",
    "\n",
    "print('accuracy on training data: ', accuracy_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_data = lrTFIDF.predict(X_test_features)\n",
    "accuracy_test_data = accuracy_score(Y_test_TFIDF, prediction_test_data)\n",
    "print('accuracy on training data: ', accuracy_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_email = [\"You have 18 unread notifications to review\"]\n",
    "input_data_features = feature_extraciton.transform(input_email)\n",
    "\n",
    "prediction = lrTFIDF.predict(input_data_features)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0]==1):\n",
    "    print('Spam mail')\n",
    "\n",
    "else:\n",
    "    print('Ham mail')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
