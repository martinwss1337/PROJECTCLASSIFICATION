{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install lxml\n",
    "%pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_30852\\2672943375.py:35: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(s, 'lxml')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wrong bill grace forward original message rodr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>continued hilcorp old ocean deal dan hyvl writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>several related issue resulted increase level ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>one year rate one mm volume greater mm day pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>attached weekly deal report lex carroll enron ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             origin\n",
       "0      0  wrong bill grace forward original message rodr...\n",
       "1      0  continued hilcorp old ocean deal dan hyvl writ...\n",
       "2      0  several related issue resulted increase level ...\n",
       "3      0  one year rate one mm volume greater mm day pri...\n",
       "4      0  attached weekly deal report lex carroll enron ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import email\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('email_origin.csv')\n",
    "\n",
    "def read_email_from_string(s):\n",
    "    message = email.message_from_string(s)\n",
    "    return message\n",
    "\n",
    "def extract_email_body(message):\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            type_content = part.get_content_maintype()\n",
    "            if type_content == 'text':\n",
    "                message = part\n",
    "                break\n",
    "        else:\n",
    "            return ''\n",
    "    body = message.get_payload(decode=False)\n",
    "    return body\n",
    "\n",
    "def remove_html(s):\n",
    "    soup = BeautifulSoup(s, 'lxml')\n",
    "    for sp in soup(['script', 'style', 'head', 'meta', 'noscript']):\n",
    "        sp.decompose()\n",
    "    s = ' '.join(soup.stripped_strings)\n",
    "    return s\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Extract email body\n",
    "    body = extract_email_body(read_email_from_string(text))\n",
    "    body = remove_html(body)\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(body)\n",
    "\n",
    "    # Remove punctuation and convert to lower case\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    words = [word for word in words if word != 'subject']\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['origin'] = df['origin'].apply(preprocess_text)\n",
    "\n",
    "# Save the refactored emails back to the same file\n",
    "df.to_csv('email_origin.csv', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26961"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1, random_state = 1)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Create split index for 80/20 split\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = df[:split_index], df[split_index:]\n",
    "df = train_df.reset_index(drop = True)\n",
    "df = test_df.reset_index(drop = True)\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens in training set\n",
    "\n",
    "token_counter = {}\n",
    "for message in train_df['origin']:\n",
    "  words = nltk.word_tokenize(message)\n",
    "\n",
    "  for token in words:\n",
    "    if token in token_counter:\n",
    "      token_counter[token] += 1\n",
    "    else:\n",
    "      token_counter[token] = 1\n",
    "\n",
    "token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_token(proccessed_token, threshold):\n",
    "  if proccessed_token not in token_counter:\n",
    "    return False\n",
    "  else:\n",
    "    # Add condition to check length of token\n",
    "    return token_counter[proccessed_token] > threshold and len(proccessed_token) > 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the current dataset 500 is a good value which generates a\n",
    "# set with an acceptable length to be considered a feature set for the\n",
    "# machine learning algorithms\n",
    "\n",
    "# this is the Bag of Words approach\n",
    "\n",
    "features = set()\n",
    "\n",
    "for token in token_counter:\n",
    "  if keep_token(token, 8000):\n",
    "    features.add(token)\n",
    "\n",
    "features = list(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "  count_vector = np.zeros(len(features))\n",
    "\n",
    "  processed_list_of_tokens = nltk.word_tokenize(message)\n",
    "\n",
    "  for token in processed_list_of_tokens:\n",
    "    if token not in features:\n",
    "      continue\n",
    "    index = token_to_index_mapping[token]\n",
    "    count_vector[index] += 1\n",
    "\n",
    "  return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_to_count_vector(train_df['origin'].iloc[9010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                                     1\n",
       "origin    hello viagra med struggle men erectile dysfunc...\n",
       "Name: 9010, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 spam; 0 not spam\n",
    "train_df.iloc[9010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(dataframe):\n",
    "  # Extract labels and convert to integer type\n",
    "  labels = dataframe['label'].values.astype(int)\n",
    "\n",
    "  # Extract messages\n",
    "  messages = dataframe['origin']\n",
    "  vector_counts = []\n",
    "\n",
    "  # Convert each message to a count vector\n",
    "  for msg in messages:\n",
    "    vector = message_to_count_vector(msg)\n",
    "    vector_counts.append(vector)\n",
    "\n",
    "  # Convert list of count vectors to a numpy array and cast to integer type\n",
    "  feature_matrix = np.asarray(vector_counts).astype(int)\n",
    "\n",
    "  return feature_matrix, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26961, 26), (26961,), (6741, 26), (6741,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_BOW, Y_train_BOW = extract_features_and_labels(train_df)\n",
    "\n",
    "X_test_BOW, Y_test_BOW = extract_features_and_labels(test_df)\n",
    "\n",
    "X_train_BOW.shape, Y_train_BOW.shape, X_test_BOW.shape, Y_test_BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.00133869, ..., 0.00142653, 0.        ,\n",
       "        0.01851852],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.04878049,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_BOW)\n",
    "\n",
    "X_train_BOW, X_test_BOW = scaler.transform(X_train_BOW), scaler.transform(X_test_BOW)\n",
    "\n",
    "X_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      3300\n",
      "           1       0.81      0.81      0.81      3441\n",
      "\n",
      "    accuracy                           0.80      6741\n",
      "   macro avg       0.80      0.80      0.80      6741\n",
      "weighted avg       0.80      0.80      0.80      6741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lrBOW = LogisticRegression().fit(X_train_BOW, Y_train_BOW)\n",
    "print(classification_report(Y_test_BOW, lrBOW.predict(X_test_BOW)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['origin']\n",
    "Y = df['label']\n",
    "X_train_TFIDF, X_test_TFIDF, Y_train_TFIDF, Y_test_TFIDF = train_test_split(X, Y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraciton = TfidfVectorizer(min_df = 1, stop_words='english')\n",
    "X_train_features = feature_extraciton.fit_transform(X_train_TFIDF)\n",
    "X_test_features = feature_extraciton.transform(X_test_TFIDF)\n",
    "\n",
    "Y_train_TFIDF = Y_train_TFIDF.astype('int')\n",
    "Y_test_TFIDF = Y_test_TFIDF.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data:  0.9842359050445104\n"
     ]
    }
   ],
   "source": [
    "lrTFIDF = LogisticRegression()\n",
    "\n",
    "lrTFIDF.fit(X_train_features, Y_train_TFIDF)\n",
    "\n",
    "prediction_training_data = lrTFIDF.predict(X_train_features)\n",
    "\n",
    "accuracy_training_data = accuracy_score(Y_train_TFIDF, prediction_training_data)\n",
    "\n",
    "print('accuracy on training data: ', accuracy_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data:  0.9710896960711638\n"
     ]
    }
   ],
   "source": [
    "prediction_test_data = lrTFIDF.predict(X_test_features)\n",
    "accuracy_test_data = accuracy_score(Y_test_TFIDF, prediction_test_data)\n",
    "print('accuracy on training data: ', accuracy_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Ham mail\n"
     ]
    }
   ],
   "source": [
    "input_email = [\"You have 18 unread notifications to review\"]\n",
    "input_data_features = feature_extraciton.transform(input_email)\n",
    "\n",
    "prediction = lrTFIDF.predict(input_data_features)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0]==1):\n",
    "    print('Spam mail')\n",
    "\n",
    "else:\n",
    "    print('Ham mail')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
